{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q1"
      ],
      "metadata": {
        "id": "wQZ8Y3Kknqc9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcc1qWzjl6cT"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "full_tree = DecisionTreeClassifier()\n",
        "full_tree.fit(X_train, y_train)\n",
        "#Bashouf el accuracy 3ala el training w el testing\n",
        "\n",
        "train_acc_full = full_tree.score(X_train, y_train)\n",
        "test_acc_full = full_tree.score(X_test, y_test)\n",
        "\n",
        "print(\"Full Decision Tree:\")\n",
        "print(\"Train Accuracy:\", train_acc_full)\n",
        "print(\"Test Accuracy :\", test_acc_full)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "\n",
        "plot_tree(full_tree, filled=True, fontsize=6)\n",
        "plt.show()\n",
        "#This tree is limited to depth 3 which is less overfitting\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "\n",
        "train_acc_pruned = pruned_tree.score(X_train, y_train)\n",
        "test_acc_pruned = pruned_tree.score(X_test, y_test)\n",
        "\n",
        "print(\"nPruned Decision Tree:\")\n",
        "print(\"Train Accuracy:\", train_acc_pruned)\n",
        "print(\"Test Accuracy :\", test_acc_pruned)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The full tree learns too much and overfits, so train accuracy is high but test accuracy is so low.\n",
        "The pruned tree is simpler and more balanced, giving better generalization on test data."
      ],
      "metadata": {
        "id": "5g6Za1FSpzNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2"
      ],
      "metadata": {
        "id": "rW2yh1P-nm8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "#Creates 100 decision trees and combines them\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "#Zay el tree, bas accuracy a7san and more stable\n",
        "rf_train_acc = rf.score(X_train, y_train)\n",
        "rf_test_acc = rf.score(X_test, y_test)\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(\"Train Accuracy:\", rf_train_acc)\n",
        "print(\"Test Accuracy :\", rf_test_acc)\n",
        "\n",
        "print(\"nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, rf.predict(X_test)))\n",
        "\n",
        "print(\"nClassification Report:\")\n",
        "print(classification_report(y_test, rf.predict(X_test)))\n",
        "\n",
        "print(\"nComparison:\")\n",
        "print(\"Full Tree Test Acc   :\", test_acc_full)\n",
        "print(\"Pruned Tree Test Acc :\", test_acc_pruned)\n",
        "print(\"Random Forest Test Acc:\", rf_test_acc)\n"
      ],
      "metadata": {
        "id": "aYcT_8IkmAH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest is more stable and accurate because it uses many trees together.\n",
        "It reduces overfitting and usually performs better than a single decision tree"
      ],
      "metadata": {
        "id": "Pyoztk9lpsfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3"
      ],
      "metadata": {
        "id": "GvyFZFGXni3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "#Builds trees one by one, each tree fixes mistakes of the previous one\n",
        "gb_default = GradientBoostingClassifier()\n",
        "gb_default.fit(X_train, y_train)\n",
        "\n",
        "print(\"Gradient Boosting:\")\n",
        "print(\"Train Accuracy:\", gb_default.score(X_train, y_train))\n",
        "print(\"Test Accuracy :\", gb_default.score(X_test, y_test))\n",
        "#We try different settings and see which is best\n",
        "learning_rates = [0.01, 0.1]\n",
        "estimators = [50, 100, 200]\n",
        "\n",
        "print(\"nTesting Learning Rates and Estimators:\")\n",
        "for lr in learning_rates:\n",
        "    for n in estimators:\n",
        "        gb = GradientBoostingClassifier(learning_rate=lr, n_estimators=n)\n",
        "        gb.fit(X_train, y_train)\n",
        "        print(\"nlearning_rate =\", lr, \" n_estimators\", n)\n",
        "        print(\"Train Accuracy:\", gb.score(X_train, y_train))\n",
        "        print(\"Test Accuracy :\", gb.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "nIFMCEUMmDaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of estimators makes the model stronger but may overfit.\n",
        "Lower learning rate makes training slower but usually improves generalization."
      ],
      "metadata": {
        "id": "6h-cUkzKpn34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4"
      ],
      "metadata": {
        "id": "bSEdk6pjmJCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"nTop 5 Feature Importances (Random Forest):\")\n",
        "#Shows which features are most important in predictions\n",
        "rf_importances = rf.feature_importances_\n",
        "top5_rf = np.argsort(rf_importances)[-5:]\n",
        "for i in top5_rf:\n",
        "    print(data.feature_names[i], \":\", rf_importances[i])\n",
        "#take the last 5 = biggest 5 values\n",
        "gb_importances = gb_default.feature_importances_\n",
        "top5_gb = np.argsort(gb_importances)[-5:]\n",
        "\n",
        "print(\"nTop 5 Feature Importances (Gradient Boosting):\")\n",
        "for i in top5_gb:\n",
        "    print(data.feature_names[i], \":\", gb_importances[i])\n"
      ],
      "metadata": {
        "id": "Zx27xbSjmFYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest and Gradient Boosting both highlight similar important features\n",
        "\n",
        "Gradient Boosting often gives slightly clearer importance ranking"
      ],
      "metadata": {
        "id": "SiDIMLjOpgxU"
      }
    }
  ]
}